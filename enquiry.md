Q-learning术语时序差分算法，使用TD(0)更新规则，其中"0"表示只使用一步前瞻，即更新公式为：Q(s,a) ← Q(s,a) + α[r + γ max_a' Q(s',a') - Q(s,a)]
同时也是model-free算法，所以不需要环境的状态转移概率P(s'|s,a)或奖励函数R(s,a,s')的显式知识，仅通过与环境交互采样得到的经验元组(s,a,r,s')来更新动作价值函数估计Q(s,a)，使Q(s,a)收敛到最优动作价值函数Q*(s,a)
其中与环境交互采样使用的behavior policy都是有探索性的(比如: ε-greedy)，而最终的target policy则是根据最优动作价值函数所得到的贪婪策略

仔细思考，给我的回答要专业且非常严谨，不能容忍任何错误